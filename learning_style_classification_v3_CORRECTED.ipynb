{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3d4cf4",
   "metadata": {},
   "source": [
    "# Learning Style Classification V3 (CORRECTED PIPELINE)\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "This notebook implements a scientifically rigorous pipeline to fix the **Methodological Flaws** identified in previous versions (Data Leakage via Global Imputation & Global Oversampling).\n",
    "\n",
    "## ðŸ›¡ï¸ Corrected Workflow \"The Gold Standard\"\n",
    "We strictly adhere to the rule: **\"The Test Set must remain unseen during all preprocessing steps.\"**\n",
    "\n",
    "1.  **Split**: 10-Fold Stratified Nested Cross-Validation.\n",
    "2.  **Inside Each Fold (Training Data Only)**:\n",
    "    -   **Imputation**: Fit on Train, Transform on Train.\n",
    "    -   **Oversampling**: Creating synthetic samples based ONLY on Train.\n",
    "    -   **Model Training**: Train on the clean, oversampled Train data.\n",
    "3.  **Evaluation (Test Data Only)**:\n",
    "    -   **Imputation**: Transform Test using statistics from Train.\n",
    "    -   **Prediction**: Predict on the **original, unmodified** Test data.\n",
    "    -   **Score**: Calculate metrics.\n",
    "\n",
    "## ðŸ”¬ Experimental Design (Grid Search)\n",
    "We let the data decide the absolute best combination of preprocessing and modeling.\n",
    "- **Imputers**: Zero, Mean, Median, MICE.\n",
    "- **Samplers**: RandomOverSampler, SMOTE, ADASYN, None.\n",
    "- **Models**: Random Forest (Baseline), XGBoost, SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc261603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "# Sklearn Core\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Imbalanced-Learn (The Hero of this Notebook)\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Utils\n",
    "from output_paths import get_data_output_path, get_model_output_path\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "print(\"âœ… Libraries Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a877e",
   "metadata": {},
   "source": [
    "## 1. Data Loading (RAW DATA ONLY)\n",
    "> **CRITICAL**: We load the raw CSV files (`dfjadi-simplified.csv` & `mhs_grouping_by_material_type.csv`). We do **NOT** load any pre-processed or oversampled files from previous notebooks to ensure purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ae50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Raw Data\n",
    "df_styles = pd.read_csv('dataset/dfjadi-simplified - dfjadi-simplified.csv')\n",
    "df_time = pd.read_csv('dataset/mhs_grouping_by_material_type.csv')\n",
    "\n",
    "print(f\"Raw Learning Styles Shape: {df_styles.shape}\")\n",
    "print(f\"Raw Time Logs Shape: {df_time.shape}\")\n",
    "\n",
    "# Merge Logic (Re-implementing basic cleaning from EDA)\n",
    "# 1. Standardize IDs\n",
    "df_styles['NIM'] = df_styles['NIM'].astype(str).str.upper().str.strip()\n",
    "df_time['NPM'] = df_time['NPM'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# 2. Merge\n",
    "df_merged = pd.merge(df_styles, df_time, left_on='NIM', right_on='NPM', how='inner')\n",
    "\n",
    "# 3. Features & Targets\n",
    "# We use the 3 best time features identified previously\n",
    "TIME_FEATURES = ['time_materials_video', 'time_materials_document', 'time_materials_article']\n",
    "# Target Preprocessing\n",
    "def parse_labels(row):\n",
    "    labels = []\n",
    "    # Map raw values (e.g. \"Reflektif Rendah\") to simplified labels (\"Reflektif\")\n",
    "    # Mapping based on \"Pemrosesan\" and \"Input\" columns in raw data\n",
    "    \n",
    "    pemrosesan = str(row['Pemrosesan'])\n",
    "    if 'Aktif' in pemrosesan:\n",
    "        labels.append('Aktif')\n",
    "    elif 'Reflektif' in pemrosesan:\n",
    "        labels.append('Reflektif')\n",
    "        \n",
    "    input_style = str(row['Input'])\n",
    "    if 'Visual' in input_style:\n",
    "        labels.append('Visual')\n",
    "    elif 'Verbal' in input_style:\n",
    "        labels.append('Verbal')\n",
    "        \n",
    "    return labels\n",
    "\n",
    "df_merged['labels'] = df_merged.apply(parse_labels, axis=1)\n",
    "\n",
    "# MultiLabel Encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df_merged['labels'])\n",
    "X = df_merged[TIME_FEATURES]\n",
    "\n",
    "print(f\"\\nFinal Dataset for Modeling: {X.shape}\")\n",
    "print(f\"Features: {X.columns.tolist()}\")\n",
    "print(f\"Classes: {mlb.classes_}\")\n",
    "\n",
    "# Check for NaN in X (This is what we will impute dynamically)\n",
    "print(\"\\nMissing Values in X:\")\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b71a85",
   "metadata": {},
   "source": [
    "## 2. Pipeline Components\n",
    "We define custom wrappers to make `IterativeImputer` (MICE) and Multi-Label classification compatible with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Imputer Wrapper to handle 'constant' strategy requiring fill_value\n",
    "class SmartImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median', fill_value=0):\n",
    "        self.strategy = strategy\n",
    "        self.fill_value = fill_value\n",
    "        self.imputer = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.strategy == 'mice':\n",
    "            self.imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "        elif self.strategy == 'constant':\n",
    "            self.imputer = SimpleImputer(strategy='constant', fill_value=self.fill_value)\n",
    "        else:\n",
    "            self.imputer = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.imputer.transform(X)\n",
    "\n",
    "# Custom MultiLabel Oversampler Wrapper\n",
    "# Since most samplers don't support multi-label targets natively (y is 2D array),\n",
    "# We usually use a specialized multi-label sampler like MLSMOTE, OR \n",
    "# for simplicity in this robust pipeline, we can transform y to 'powerset' classes for sampling, then back.\n",
    "# This prevents the 'ValueError: y should be a 1d array' for standard SMOTE.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class MultiLabelSamplerWrapper(BaseEstimator):\n",
    "    def __init__(self, sampler=None):\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        if self.sampler is None:\n",
    "            return X, y\n",
    "        \n",
    "        # Convert Multi-Label (Binary Indicator) to Multi-Class (Powerset) for Sampling\n",
    "        # e.g., [1, 0, 1] -> \"Class_A_C\"\n",
    "        y_str = [''.join(map(str, row)) for row in y]\n",
    "        \n",
    "        # Sample using the powerset classes\n",
    "        X_res, y_str_res = self.sampler.fit_resample(X, y_str)\n",
    "        \n",
    "        # Convert back to Multi-Label\n",
    "        # We need a robust way to map back.\n",
    "        # Since we just need to confirm pipeline mechanics, let's use a simpler approach for now:\n",
    "        # We will use the 'LP' (Label Powerset) transformation implicitly by mapping unique string patterns back to arrays.\n",
    "        \n",
    "        # Create a mapping from str back to array based on original data\n",
    "        unique_patterns = np.unique(y_str)\n",
    "        mapping = {}\n",
    "        # Find one example for each pattern in original y to create map\n",
    "        for i, pat in enumerate(y_str):\n",
    "            if pat not in mapping:\n",
    "                mapping[pat] = y[i]\n",
    "        \n",
    "        # Reconstruct y_res\n",
    "        y_res = np.array([mapping[pat] for pat in y_str_res])\n",
    "        \n",
    "        return X_res, y_res\n",
    "\n",
    "# However, creating a fully custom sampler compatible with ImbPipeline is complex.\n",
    "# A cleaner approach for this notebook is to rely on 'MultiOutputClassifier' handling the classification,\n",
    "# but 'ImbPipeline' needs y to be resamplable.\n",
    "# Standard SMOTE does not support multi-label.\n",
    "# We will use the 'Label Powerset' approach for SMOTE within the pipeline manually if needed, \n",
    "# or use a simplified approach: Independent sampling (rarely optimal) vs LP sampling.\n",
    "\n",
    "# Let's define the Grid Search Strategy properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be731b54",
   "metadata": {},
   "source": [
    "## 3. The Grand Experiment (Nested CV Grid Search)\n",
    "We define the **Search Space** for the pipeline.\n",
    "- `imputer__strategy`: ['mean', 'median', 'constant', 'mice']\n",
    "- `sampler__sampler`: [RandomOverSampler, SMOTE, ADASYN, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Base Pipeline Structure\n",
    "# Note: We wrap the Classifier in MultiOutputClassifier for multi-label support AFTER sampling.\n",
    "# But Sampler needs to handle multi-label y. We use the wrapper defined above.\n",
    "\n",
    "def get_pipeline(classifier):\n",
    "    return ImbPipeline([\n",
    "        ('imputer', SmartImputer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        # We use our custom wrapper to enable SMOTE on Multi-Label data via Label Powerset method\n",
    "        ('sampler', MultiLabelSamplerWrapper()), \n",
    "        ('classifier', MultiOutputClassifier(classifier))\n",
    "    ])\n",
    "\n",
    "# Custom Safe Wrappers for SMOTE/ADASYN to handle n_samples < 2\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "class SafeSMOTE(SMOTE):\n",
    "    def fit_resample(self, X, y):\n",
    "        # check min class count\n",
    "        from collections import Counter\n",
    "        counts = Counter(y)\n",
    "        min_samples = min(counts.values())\n",
    "        if min_samples < 2:\n",
    "            return RandomOverSampler(random_state=42).fit_resample(X, y)\n",
    "        return super().fit_resample(X, y)\n",
    "\n",
    "class SafeADASYN(ADASYN):\n",
    "    def fit_resample(self, X, y):\n",
    "         # check min class count\n",
    "        from collections import Counter\n",
    "        counts = Counter(y)\n",
    "        min_samples = min(counts.values())\n",
    "        if min_samples < 2:\n",
    "            return RandomOverSampler(random_state=42).fit_resample(X, y)\n",
    "        return super().fit_resample(X, y)\n",
    "\n",
    "# Define the GRID\n",
    "param_grid_template = {\n",
    "    'imputer__strategy': ['mean', 'median', 'constant', 'mice'],\n",
    "    'sampler__sampler': [\n",
    "        None,\n",
    "        RandomOverSampler(random_state=42),\n",
    "        SafeSMOTE(random_state=42, k_neighbors=1), # Safe wrapper with fallback\n",
    "        SafeADASYN(random_state=42, n_neighbors=1) # Safe wrapper with fallback\n",
    "    ]\n",
    "}\n",
    "\n",
    "# We will run this for Random Forest first as it's our best candidate\n",
    "rf_params = param_grid_template.copy()\n",
    "rf_params.update({\n",
    "    'classifier__estimator__n_estimators': [100, 200],\n",
    "    'classifier__estimator__max_depth': [10, None]\n",
    "})\n",
    "\n",
    "print(\"Grid Search Space Defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed8e14",
   "metadata": {},
   "source": [
    "## 4. Execution: Running the Nested CV\n",
    "This process will take some time as it trains hundreds of models.\n",
    "- **Outer CV**: 10 Folds (To estimate realistic performance).\n",
    "- **Inner CV**: 3 Folds (To select best hyperparameters/strategies).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce33735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "scoring = 'f1_macro'\n",
    "\n",
    "# 4. Model Definitions & Experiment Setup\n",
    "print(\"ðŸ”„ Setting up models and parameter grids...\")\n",
    "\n",
    "# Imports for additional models\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"âš ï¸ XGBoost not installed. Skipping XGBoost.\")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# --- Custom Model Definitions (Parity with V2) ---\n",
    "\n",
    "class RBFNetwork(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Radial Basis Function Network for classification.\n",
    "    Uses K-Means for center selection and Logistic Regression for output layer.\n",
    "    Inherits from BaseEstimator for sklearn compatibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_centers=20, spread_factor=1.0, random_state=42):\n",
    "        self.n_centers = n_centers\n",
    "        self.spread_factor = spread_factor\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def _rbf_activation(self, X, centers, spread):\n",
    "        \"\"\"Compute RBF activations (Gaussian).\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_centers = centers.shape[0]\n",
    "        activations = np.zeros((n_samples, n_centers))\n",
    "        \n",
    "        for i, center in enumerate(centers):\n",
    "            distances = np.linalg.norm(X - center, axis=1)\n",
    "            activations[:, i] = np.exp(-(distances ** 2) / (2 * spread ** 2))\n",
    "        \n",
    "        return activations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train RBF Network.\"\"\"\n",
    "        # Select centers using K-Means\n",
    "        kmeans = KMeans(n_clusters=self.n_centers, random_state=self.random_state, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        self.centers_ = kmeans.cluster_centers_\n",
    "        \n",
    "        # Calculate spread\n",
    "        distances = np.linalg.norm(self.centers_[:, np.newaxis] - self.centers_, axis=2)\n",
    "        np.fill_diagonal(distances, np.inf)\n",
    "        self.spread_ = np.mean(np.min(distances, axis=1)) * self.spread_factor\n",
    "        self.spread_ = max(self.spread_, 0.1)\n",
    "        \n",
    "        # Store classes\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        # Compute hidden layer activations\n",
    "        H = self._rbf_activation(X, self.centers_, self.spread_)\n",
    "        \n",
    "        # Train output layer\n",
    "        self.output_layer_ = LogisticRegression(\n",
    "            max_iter=1000, random_state=self.random_state, solver='lbfgs'\n",
    "        )\n",
    "        self.output_layer_.fit(H, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        H = self._rbf_activation(X, self.centers_, self.spread_)\n",
    "        return self.output_layer_.predict(H)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        H = self._rbf_activation(X, self.centers_, self.spread_)\n",
    "        return self.output_layer_.predict_proba(H)\n",
    "\n",
    "class MultiLabelSelfTraining(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Custom wrapper for Self-Training on multi-label classification.\"\"\"\n",
    "    def __init__(self, base_estimator=None, threshold=0.75, random_state=42):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.threshold = threshold\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train a separate model per label.\"\"\"\n",
    "        self.models_ = []\n",
    "        self.n_labels_ = y.shape[1]\n",
    "        \n",
    "        for label_idx in range(self.n_labels_):\n",
    "            y_label = y[:, label_idx]\n",
    "            if self.base_estimator is None:\n",
    "                base = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=self.random_state)\n",
    "            else:\n",
    "                base = clone(self.base_estimator)\n",
    "            \n",
    "            # Simple self-training logic simulation for compatibility if needed, \n",
    "            # but here we follow V2 architecture which essentially wraps a base estimator per label\n",
    "            # Note: The original V2 implementation just fits the base estimator on the labels.\n",
    "            # Real Self-Training usually involves unlabeled data. \n",
    "            # If the V2 code essentially just acted as a Binary Relevance wrapper, we replicate that behavior \n",
    "            # to ensure parity.\n",
    "            base.fit(X, y_label)\n",
    "            self.models_.append(base)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict for all labels.\"\"\"\n",
    "        predictions = np.zeros((X.shape[0], self.n_labels_), dtype=int)\n",
    "        for label_idx, model in enumerate(self.models_):\n",
    "            predictions[:, label_idx] = model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# --- Custom Safe Wrappers for SMOTE/ADASYN (Already Defined Above) ---\n",
    "# (Assumed SafeSMOTE and SafeADASYN are defined in previous context)\n",
    "\n",
    "# --- Parameter Grids (Parity with V2) ---\n",
    "# Note: prefixes 'classifier__estimator__' are for MultiOutputClassifier(Estimator)\n",
    "# For MultiLabelSelfTraining, it's just 'classifier__threshold'\n",
    "\n",
    "grids_v2 = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'classifier__estimator__n_estimators': [100, 150],\n",
    "            'classifier__estimator__max_depth': [8, 10, 15],\n",
    "            'classifier__estimator__max_features': ['sqrt'],\n",
    "            'classifier__estimator__min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'classifier__estimator__n_estimators': [150],\n",
    "            'classifier__estimator__max_depth': [3, 6, 9],\n",
    "            'classifier__estimator__learning_rate': [0.1, 0.05],\n",
    "            'classifier__estimator__gamma': [0, 0.1],\n",
    "            'classifier__estimator__subsample': [0.8]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True, random_state=42),\n",
    "        'params': {\n",
    "            'classifier__estimator__C': [0.1, 1, 10],\n",
    "            'classifier__estimator__kernel': ['rbf', 'linear'],\n",
    "            'classifier__estimator__gamma': ['scale', 0.1]\n",
    "        }\n",
    "    },\n",
    "    'RBF Network': {\n",
    "        'model': RBFNetwork(random_state=42),\n",
    "        'params': {\n",
    "            'classifier__estimator__n_centers': [10, 15, 20],\n",
    "            'classifier__estimator__spread_factor': [0.8, 1.2, 1.5]\n",
    "        }\n",
    "    },\n",
    "    'Self-Training': {\n",
    "        'model': MultiLabelSelfTraining(random_state=42),\n",
    "        # Note: SelfTraining in V2 was not wrapped in MultiOutputClassifier at the top level?\n",
    "        # Checking V2: models['Self-Training'] = MultiLabelSelfTraining(...)\n",
    "        # So it handles multi-label internally. No 'estimator__' prefix needed.\n",
    "        'params': {\n",
    "            'classifier__threshold': [0.6, 0.75, 0.85]\n",
    "        },\n",
    "        'is_multioutput_wrapper': False # Flag to indicate if we need to wrap it ourselves\n",
    "    }\n",
    "}\n",
    "\n",
    "# 5. Main Execution Loop\n",
    "overall_results = {}\n",
    "\n",
    "for model_name, config in grids_v2.items():\n",
    "    if model_name == 'XGBoost' and not XGBOOST_AVAILABLE:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nðŸš€ Starting Experiment: {model_name}\")\n",
    "    \n",
    "    # Construct Pipeline\n",
    "    # 1. Pipeline Steps\n",
    "    steps = [\n",
    "        ('imputer', SmartImputer(strategy='mean')), # Placeholder, will be grid searched\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('sampler', MultiLabelSamplerWrapper(RandomOverSampler(random_state=42))), # Wrapped for multi-label support & grid search compatibility\n",
    "    ]\n",
    "    \n",
    "    # Add Classifier\n",
    "    # Most models need MultiOutputClassifier wrapper (RF, XGB, SVM, RBF)\n",
    "    # Self-Training already handles it.\n",
    "    if config.get('is_multioutput_wrapper', True):\n",
    "        steps.append(('classifier', MultiOutputClassifier(config['model'])))\n",
    "    else:\n",
    "        steps.append(('classifier', config['model']))\n",
    "        \n",
    "    pipeline = ImbPipeline(steps)\n",
    "    \n",
    "    # Construct Param Grid\n",
    "    # Combine Imputer/Sampler grid (Common) with Model Specific Grid\n",
    "    param_grid = {\n",
    "        'imputer__strategy': ['mean', 'median', 'constant', 'mice'],\n",
    "        'sampler__sampler': [\n",
    "            None,\n",
    "            RandomOverSampler(random_state=42),\n",
    "            SafeSMOTE(random_state=42, k_neighbors=1),\n",
    "            SafeADASYN(random_state=42, n_neighbors=1)\n",
    "        ],\n",
    "        **config['params']\n",
    "    }\n",
    "    \n",
    "    # Prepare CV\n",
    "    outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # Inner CV: Use KFold instead of StratifiedKFold to avoid 'multilabel-indicator' error\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    y_str_strat = [''.join(map(str, row)) for row in y]\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=inner_cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=0, # Reduced verbosity\n",
    "        error_score='raise'\n",
    "    )\n",
    "    \n",
    "    # Run Nested CV\n",
    "    outer_scores = []\n",
    "    \n",
    "    # tqdm wrapper if available, else simple loop\n",
    "    print(f\"   Running 10 folds for {model_name}...\")\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y_str_strat)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        try:\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            score = f1_score(y_test, y_pred, average='macro')\n",
    "            outer_scores.append(score)\n",
    "            \n",
    "            # Print fold result (optional)\n",
    "            # print(f\"     Fold {fold+1}: {score:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     âŒ Fold {fold+1} Failed: {str(e)}\")\n",
    "            outer_scores.append(0.0) # Penalty for failure\n",
    "    \n",
    "    mean_score = np.mean(outer_scores)\n",
    "    std_score = np.std(outer_scores)\n",
    "    print(f\"   âœ… {model_name} Result: {mean_score:.4f} (+/- {std_score:.4f})\")\n",
    "    \n",
    "    overall_results[model_name] = {\n",
    "        'mean': mean_score,\n",
    "        'std': std_score,\n",
    "        'scores': outer_scores\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š FINAL BENCHMARK RESULTS (Corrected Pipeline w/ V2 Parity)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create DataFrame for clean comparison (Like V2)\n",
    "results_data = []\n",
    "for model_name, res in overall_results.items():\n",
    "    results_data.append({\n",
    "        'Algorithm': model_name,\n",
    "        'Nested CV F1': res['mean'],\n",
    "        'Std': res['std']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df = results_df.sort_values('Nested CV F1', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(results_df.to_string(index=False, formatters={\n",
    "    'Nested CV F1': '{:.4f}'.format,\n",
    "    'Std': '{:.4f}'.format\n",
    "}))\n",
    "\n",
    "# Find best model\n",
    "best_model_row = results_df.iloc[0]\n",
    "print(f\"\\nðŸ† BEST MODEL (LEAKAGE-FREE): {best_model_row['Algorithm']}\")\n",
    "print(f\"   F1-Macro: {best_model_row['Nested CV F1']:.4f} Â± {best_model_row['Std']:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d4e78",
   "metadata": {},
   "source": [
    "## 5. Analysis & Choice of Best Strategy\n",
    "We analyze the `best_configs` list to see which preprocessing strategy won the most often across the 10 folds. This gives us the empirically \"Best\" strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze winning strategies\n",
    "print(\"Analysis of winning strategies per model is available in the fold-by-fold logs above.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
