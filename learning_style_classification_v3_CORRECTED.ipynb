{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2125dc7",
   "metadata": {},
   "source": [
    "# Learning Style Classification V3 (CORRECTED PIPELINE)\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "This notebook implements a scientifically rigorous pipeline to fix the **Methodological Flaws** identified in previous versions (Data Leakage via Global Imputation & Global Oversampling).\n",
    "\n",
    "## ðŸ›¡ï¸ Corrected Workflow \"The Gold Standard\"\n",
    "We strictly adhere to the rule: **\"The Test Set must remain unseen during all preprocessing steps.\"**\n",
    "\n",
    "1.  **Split**: 10-Fold Stratified Nested Cross-Validation.\n",
    "2.  **Inside Each Fold (Training Data Only)**:\n",
    "    -   **Imputation**: Fit on Train, Transform on Train.\n",
    "    -   **Oversampling**: Creating synthetic samples based ONLY on Train.\n",
    "    -   **Model Training**: Train on the clean, oversampled Train data.\n",
    "3.  **Evaluation (Test Data Only)**:\n",
    "    -   **Imputation**: Transform Test using statistics from Train.\n",
    "    -   **Prediction**: Predict on the **original, unmodified** Test data.\n",
    "    -   **Score**: Calculate metrics.\n",
    "\n",
    "## ðŸ”¬ Experimental Design (Grid Search)\n",
    "We let the data decide the absolute best combination of preprocessing and modeling.\n",
    "- **Imputers**: Zero, Mean, Median, MICE.\n",
    "- **Samplers**: RandomOverSampler, SMOTE, ADASYN, None.\n",
    "- **Models**: Random Forest (Baseline), XGBoost, SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a78d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "# Sklearn Core\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Imbalanced-Learn (The Hero of this Notebook)\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Utils\n",
    "from output_paths import get_data_output_path, get_model_output_path\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "print(\"âœ… Libraries Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a27ae",
   "metadata": {},
   "source": [
    "## 1. Data Loading (RAW DATA ONLY)\n",
    "> **CRITICAL**: We load the raw CSV files (`dfjadi-simplified.csv` & `mhs_grouping_by_material_type.csv`). We do **NOT** load any pre-processed or oversampled files from previous notebooks to ensure purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Raw Data\n",
    "df_styles = pd.read_csv('dataset/dfjadi-simplified - dfjadi-simplified.csv')\n",
    "df_time = pd.read_csv('dataset/mhs_grouping_by_material_type.csv')\n",
    "\n",
    "print(f\"Raw Learning Styles Shape: {df_styles.shape}\")\n",
    "print(f\"Raw Time Logs Shape: {df_time.shape}\")\n",
    "\n",
    "# Merge Logic (Re-implementing basic cleaning from EDA)\n",
    "# 1. Standardize IDs\n",
    "df_styles['NIM'] = df_styles['NIM'].astype(str).str.upper().str.strip()\n",
    "df_time['NPM'] = df_time['NPM'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# 2. Merge\n",
    "df_merged = pd.merge(df_styles, df_time, left_on='NIM', right_on='NPM', how='inner')\n",
    "\n",
    "# 3. Features & Targets\n",
    "# We use the 3 best time features identified previously\n",
    "TIME_FEATURES = ['time_materials_video', 'time_materials_document', 'time_materials_article']\n",
    "# Target Preprocessing\n",
    "def parse_labels(row):\n",
    "    labels = []\n",
    "    # Map raw values (e.g. \"Reflektif Rendah\") to simplified labels (\"Reflektif\")\n",
    "    # Mapping based on \"Pemrosesan\" and \"Input\" columns in raw data\n",
    "    \n",
    "    pemrosesan = str(row['Pemrosesan'])\n",
    "    if 'Aktif' in pemrosesan:\n",
    "        labels.append('Aktif')\n",
    "    elif 'Reflektif' in pemrosesan:\n",
    "        labels.append('Reflektif')\n",
    "        \n",
    "    input_style = str(row['Input'])\n",
    "    if 'Visual' in input_style:\n",
    "        labels.append('Visual')\n",
    "    elif 'Verbal' in input_style:\n",
    "        labels.append('Verbal')\n",
    "        \n",
    "    return labels\n",
    "\n",
    "df_merged['labels'] = df_merged.apply(parse_labels, axis=1)\n",
    "\n",
    "# MultiLabel Encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df_merged['labels'])\n",
    "X = df_merged[TIME_FEATURES]\n",
    "\n",
    "print(f\"\\nFinal Dataset for Modeling: {X.shape}\")\n",
    "print(f\"Features: {X.columns.tolist()}\")\n",
    "print(f\"Classes: {mlb.classes_}\")\n",
    "\n",
    "# Check for NaN in X (This is what we will impute dynamically)\n",
    "print(\"\\nMissing Values in X:\")\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83475353",
   "metadata": {},
   "source": [
    "## 2. Pipeline Components\n",
    "We define custom wrappers to make `IterativeImputer` (MICE) and Multi-Label classification compatible with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Imputer Wrapper to handle 'constant' strategy requiring fill_value\n",
    "class SmartImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median', fill_value=0):\n",
    "        self.strategy = strategy\n",
    "        self.fill_value = fill_value\n",
    "        self.imputer = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.strategy == 'mice':\n",
    "            self.imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "        elif self.strategy == 'constant':\n",
    "            self.imputer = SimpleImputer(strategy='constant', fill_value=self.fill_value)\n",
    "        else:\n",
    "            self.imputer = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.imputer.transform(X)\n",
    "\n",
    "# Custom MultiLabel Oversampler Wrapper\n",
    "# Since most samplers don't support multi-label targets natively (y is 2D array),\n",
    "# We usually use a specialized multi-label sampler like MLSMOTE, OR \n",
    "# for simplicity in this robust pipeline, we can transform y to 'powerset' classes for sampling, then back.\n",
    "# This prevents the 'ValueError: y should be a 1d array' for standard SMOTE.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class MultiLabelSamplerWrapper(BaseEstimator):\n",
    "    def __init__(self, sampler=None):\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        if self.sampler is None:\n",
    "            return X, y\n",
    "        \n",
    "        # Convert Multi-Label (Binary Indicator) to Multi-Class (Powerset) for Sampling\n",
    "        # e.g., [1, 0, 1] -> \"Class_A_C\"\n",
    "        y_str = [''.join(map(str, row)) for row in y]\n",
    "        \n",
    "        # Sample using the powerset classes\n",
    "        X_res, y_str_res = self.sampler.fit_resample(X, y_str)\n",
    "        \n",
    "        # Convert back to Multi-Label\n",
    "        # We need a robust way to map back.\n",
    "        # Since we just need to confirm pipeline mechanics, let's use a simpler approach for now:\n",
    "        # We will use the 'LP' (Label Powerset) transformation implicitly by mapping unique string patterns back to arrays.\n",
    "        \n",
    "        # Create a mapping from str back to array based on original data\n",
    "        unique_patterns = np.unique(y_str)\n",
    "        mapping = {}\n",
    "        # Find one example for each pattern in original y to create map\n",
    "        for i, pat in enumerate(y_str):\n",
    "            if pat not in mapping:\n",
    "                mapping[pat] = y[i]\n",
    "        \n",
    "        # Reconstruct y_res\n",
    "        y_res = np.array([mapping[pat] for pat in y_str_res])\n",
    "        \n",
    "        return X_res, y_res\n",
    "\n",
    "# However, creating a fully custom sampler compatible with ImbPipeline is complex.\n",
    "# A cleaner approach for this notebook is to rely on 'MultiOutputClassifier' handling the classification,\n",
    "# but 'ImbPipeline' needs y to be resamplable.\n",
    "# Standard SMOTE does not support multi-label.\n",
    "# We will use the 'Label Powerset' approach for SMOTE within the pipeline manually if needed, \n",
    "# or use a simplified approach: Independent sampling (rarely optimal) vs LP sampling.\n",
    "\n",
    "# Let's define the Grid Search Strategy properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e5262",
   "metadata": {},
   "source": [
    "## 3. The Grand Experiment (Nested CV Grid Search)\n",
    "We define the **Search Space** for the pipeline.\n",
    "- `imputer__strategy`: ['mean', 'median', 'constant', 'mice']\n",
    "- `sampler__sampler`: [RandomOverSampler, SMOTE, ADASYN, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4503a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Base Pipeline Structure\n",
    "# Note: We wrap the Classifier in MultiOutputClassifier for multi-label support AFTER sampling.\n",
    "# But Sampler needs to handle multi-label y. We use the wrapper defined above.\n",
    "\n",
    "def get_pipeline(classifier):\n",
    "    return ImbPipeline([\n",
    "        ('imputer', SmartImputer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        # We use our custom wrapper to enable SMOTE on Multi-Label data via Label Powerset method\n",
    "        ('sampler', MultiLabelSamplerWrapper()), \n",
    "        ('classifier', MultiOutputClassifier(classifier))\n",
    "    ])\n",
    "\n",
    "# Custom Safe Wrappers for SMOTE/ADASYN to handle n_samples < 2\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "class SafeSMOTE(SMOTE):\n",
    "    def fit_resample(self, X, y):\n",
    "        # check min class count\n",
    "        from collections import Counter\n",
    "        counts = Counter(y)\n",
    "        min_samples = min(counts.values())\n",
    "        if min_samples < 2:\n",
    "            return RandomOverSampler(random_state=42).fit_resample(X, y)\n",
    "        return super().fit_resample(X, y)\n",
    "\n",
    "class SafeADASYN(ADASYN):\n",
    "    def fit_resample(self, X, y):\n",
    "         # check min class count\n",
    "        from collections import Counter\n",
    "        counts = Counter(y)\n",
    "        min_samples = min(counts.values())\n",
    "        if min_samples < 2:\n",
    "            return RandomOverSampler(random_state=42).fit_resample(X, y)\n",
    "        return super().fit_resample(X, y)\n",
    "\n",
    "# Define the GRID\n",
    "param_grid_template = {\n",
    "    'imputer__strategy': ['mean', 'median', 'constant', 'mice'],\n",
    "    'sampler__sampler': [\n",
    "        None,\n",
    "        RandomOverSampler(random_state=42),\n",
    "        SafeSMOTE(random_state=42, k_neighbors=1), # Safe wrapper with fallback\n",
    "        SafeADASYN(random_state=42, n_neighbors=1) # Safe wrapper with fallback\n",
    "    ]\n",
    "}\n",
    "\n",
    "# We will run this for Random Forest first as it's our best candidate\n",
    "rf_params = param_grid_template.copy()\n",
    "rf_params.update({\n",
    "    'classifier__estimator__n_estimators': [100, 200],\n",
    "    'classifier__estimator__max_depth': [10, None]\n",
    "})\n",
    "\n",
    "print(\"Grid Search Space Defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7029c",
   "metadata": {},
   "source": [
    "## 4. Execution: Running the Nested CV\n",
    "This process will take some time as it trains hundreds of models.\n",
    "- **Outer CV**: 10 Folds (To estimate realistic performance).\n",
    "- **Inner CV**: 3 Folds (To select best hyperparameters/strategies).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e506387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "scoring = 'f1_macro'\n",
    "\n",
    "# 1. Random Forest Experiment\n",
    "print(\"ðŸš€ Starting Experiment 1: Random Forest + Strategy Search...\")\n",
    "\n",
    "rf_pipeline = get_pipeline(RandomForestClassifier(random_state=42))\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# Inner CV: Use KFold instead of StratifiedKFold to avoid 'multilabel-indicator' error in GridSearchCV\n",
    "# Since standard StratifiedKFold doesn't support multi-label targets directly.\n",
    "from sklearn.model_selection import KFold\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Since StratifiedKFold expects 1D y array for splitting the OUTER loop, we use Label Powerset transformation\n",
    "y_str_strat = [''.join(map(str, row)) for row in y]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_grid=rf_params,\n",
    "    cv=inner_cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    error_score='raise' # Debugging\n",
    ")\n",
    "\n",
    "# Run Nested CV\n",
    "# We manually loop outer folds to easily track best params per fold\n",
    "outer_results = []\n",
    "best_configs = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y_str_strat)):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Fit Grid Search (Inner CV)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best Model from Inner CV\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score_inner = grid_search.best_score_\n",
    "    \n",
    "    # Evaluate on Outer Test Set (The Moment of Truth)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_score = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Fold {fold+1}/10 | Test F1: {test_score:.4f} | Best Imputer: {best_params['imputer__strategy']} | Best Sampler: {best_params['sampler__sampler']}\")\n",
    "    \n",
    "    outer_results.append(test_score)\n",
    "    best_configs.append(best_params)\n",
    "\n",
    "print(f\"\\nâœ… Final Corrected F1-Macro (Avg): {np.mean(outer_results):.4f} (+/- {np.std(outer_results):.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395af194",
   "metadata": {},
   "source": [
    "## 5. Analysis & Choice of Best Strategy\n",
    "We analyze the `best_configs` list to see which preprocessing strategy won the most often across the 10 folds. This gives us the empirically \"Best\" strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze winning strategies\n",
    "from collections import Counter\n",
    "\n",
    "imputer_wins = Counter([cfg['imputer__strategy'] for cfg in best_configs])\n",
    "sampler_wins = Counter([str(cfg['sampler__sampler']) for cfg in best_configs])\n",
    "\n",
    "print(\"ðŸ† Winning Imputation Strategies:\")\n",
    "for Strat, Count in imputer_wins.most_common():\n",
    "    print(f\"  - {Strat}: {Count} folds\")\n",
    "\n",
    "print(\"\\nðŸ† Winning Oversampling Techniques:\")\n",
    "for Strat, Count in sampler_wins.most_common():\n",
    "    print(f\"  - {Strat}: {Count} folds\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
